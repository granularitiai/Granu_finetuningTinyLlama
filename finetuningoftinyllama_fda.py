# -*- coding: utf-8 -*-
"""FinetuningOfTinyLlama_FDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h0M0fRY_TBgoM_7WUnTH_NOjNxtsfHnh
"""

!pip install datasets

!pip install -U bitsandbytes

import os
import gc
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    DataCollatorForLanguageModeling,
    pipeline
)
from peft import LoraConfig, get_peft_model
from getpass import getpass

if 'hf_key' not in locals():
  hf_key = getpass('Hugging Face API Key:')

dataset = load_dataset("Jaymax/FDA_Pharmaceuticals_FAQ")

print("Dataset columns:", dataset.column_names)

def preprocess_function(example):
    # Use the correct column names: "Question" and "Answer"
    return {"prompt": f"Question: {example['Question']}\nAnswer: {example['Answer']}"}

# Apply preprocessing directly on the dataset (it doesn't have a 'train' split)
dataset = dataset.map(preprocess_function)

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    result = tokenizer(
        examples["prompt"],
        truncation=True,
        max_length=256
    )
    # Create labels for causal language modeling
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    batch_size=32,  # Process in smaller batches
    remove_columns=["Question", "Answer", "prompt"]  # Remove original columns after tokenization
)

class GarbageCollectionCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:
            gc.collect()
            torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    from_tf=False,
    low_cpu_mem_usage=True,
    use_cache=False
)

model.gradient_checkpointing_enable()

lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.01,
    bias="none",
    task_type="CAUSAL_LM"
)



model = get_peft_model(model, lora_config)

for param in model.parameters():
    if param.requires_grad:
        param.requires_grad_(True)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # We're doing causal language modeling, not masked language modeling
)

training_args = TrainingArguments(
    output_dir="./fda_pharma_slm",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=3,
    fp16=True,
    save_steps=1000,
    logging_steps=200,
    report_to=[],
    max_grad_norm=0.3,
    dataloader_num_workers=0,
    dataloader_pin_memory=False,
    prediction_loss_only=True,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    learning_rate=2e-4
)

print("Available splits:", dataset.keys())
# If there's no 'train' split (e.g., if it's just a single dataset), create one
if 'train' not in dataset:
    tokenized_dataset = {'train': tokenized_dataset}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=data_collator,
    callbacks=[GarbageCollectionCallback()]
)



trainer.train()

model_save_path = "./fda_pharma_slm"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"Model and tokenizer saved to {model_save_path}")





!pip install huggingface_hub --q

!huggingface-cli login

model.push_to_hub("GranuAI/TinyLlama-1.1B-Chat_FDA_FAQ")
tokenizer.push_to_hub("GranuAI/TinyLlama-1.1B-Chat_FDA_FAQ")









messages = [
    {"role": "user", "content": "Looking at the guidelines in Small Entity Compliance Guide- Revision of the Nutrition and Supplement Facts Labels , How must I list dried extracts?"},
]
pipe = pipeline("text-generation", model="GranuAI/TinyLlama-1.1B-Chat_FDA_FAQ",max_new_tokens=1000)
pipe(messages)

